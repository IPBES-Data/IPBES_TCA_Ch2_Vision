---
title: 'Report Assessment Ch5 Subsidies Reform'
subtitle: 'Data Management Report'
author:
  - name: 
        family: Krug
        given: Rainer M.    
    id: rmk
    orcid: 0000-0002-7490-0066
    email: Rainer.Krug@Senckenberg.de, Rainer@Krugs.de
    affiliation: 
      - name: Senckenberg
        city: Frankfurt (Main)
        url: https://www.senckenberg.de/en/institutes/sbik-f/
    roles: [author, editor]
abstract: > 
  A short description what this is about.
  This is not a tracditional abstract, but rather something else ...
# keywords:
#   - aaaaa
#   - bbbbb
license: "CC BY"
copyright: 
  holder: No idea
  year: 2023
citation: 
  type: report
  container-title: IPBES Data Management Report
  doi: XXXXXX
doi: XXXXXX
version: 0.0.1

format:
    html:
        toc: true
        toc-depth: 4
        toc_expand: true
        embed-resources: true
        code-fold: true
        code-summary: 'Show the code'

---

```{r}
#| label: setup
#| include: false
set.seed(13)

library(tictoc)
library(arrow)
library(dplyr)
library(IPBES.R)
library(openalexR)
library(ggplot2)

if (!exists("params")) {
    params <- rmarkdown::yaml_front_matter("Report.qmd")$params
}


build <- as.integer(readLines("buildNo"))
build <- build + 1
writeLines(as.character(build), "buildNo")

knitr::opts_chunk$set(message = NA)


R1_st <- list()

st <- readLines(file.path("input", "R1.txt"))[1:70]
st[70] <- gsub(pattern = " OR", replacement = "", st[70])
R1_st[[1]] <- paste(collapse = " ", st)

st <- readLines(file.path("input", "R1.txt"))
st <- st[71:length(st)]
R1_st[[2]] <- paste(collapse = " ", st)

rm(st)

R2_st <- readLines(file.path("input", "R2.txt")) |>
    paste(collapse = " ")

```

## Working Title
IPBES_TCA_Ch5_Subsidies_Reform

## Code repo

[Github - private](https://github.com/IPBES-Data/IPBES_TCA_Ch5_Subsidies_Reform)

## Build No: `r build`

%The BuidNo is automatically increased by one each time the report is rendered. It is used to indicate different renderings when the version stays the same%.

# Introduction

### The following steps will be done in documented in this report:

- [x] Do asearch with `R1_st` and `R2_st` and determine the  number of hits
- [ ] Should the copplete `**R1** AND **R2**` be downloaded, or only a random subsets?



# Methods

The search terms are based on the [shared google doc](https://docs.google.com/document/d/1_FmxYVhpv2Bu2Gbbxb7cWc49f3soFvc64Qau_x2RAqI){target=_blank}. They are cleaned up for the usage in [OpenAlex](https://openalex.org/){target=_blank}.



## `R1` count from OpenAlex

The search terms is [R1](input/R1.txt){target=_blank}

```{r}
#| label: get get_R1_count
#|

R1_count <- sapply(
    R1_st,
    function(st) {
        openalexR::oa_fetch(
            title_and_abstract.search = compact(st),
            count_only = TRUE,
            verbose = TRUE
        )[, "count"]
    }
) |>
    sum()
```


## `R2` count from OpenAlex

The search terms is [R2](input/R2.txt){target=_blank}

```{r}
#| label: get get_R2_count
#|

R2_count <- openalexR::oa_fetch(
    title_and_abstract.search = compact(R2_st),
    count_only = TRUE,
    verbose = TRUE
)[, "count"]
```

## `R1 AND R2` count from OpenAlex

The search term is [R1](input/R1.txt){target=_blank} AND [R2](input/R2.txt){target=_blank}

```{r}
#| label: get get_R2_R2_count
#|

R1_R2_count <- sapply(
    R1_st,
    function(st) {
        openalexR::oa_fetch(
            title_and_abstract.search = compact(paste0("(", st, ") AND (", R2_st, ")")),
            count_only = TRUE,
            output = "list",
            verbose = TRUE
        )$count
    }
) |>
    sum()

```

## Download `R1 AND R2` Corpus

The corpus download will be stored in `data/pages` and the arrow database in `data/corpus`.

This is not on github!

The corpus can be read by running `read_corpus()` which opens the database so that then it can be fed into a `dplyr` pipeline. After most `dplyr` functions, the actual data needs to be collected via `dplyr::collect()`.

Only then is the actual data read!

Needs to be enabled by setting `eval: true` in the code block below.

```{r}
#| label: get_subsidies_corpus
#| eval: false
#|

tic()

get_corpus_pages <- function(search, pages_dir, rds_dir = "pages_publication_year=") {
    dir.create(
        path = pages_dir,
        showWarnings = FALSE,
        recursive = TRUE
    )

    years <- oa_fetch(
        title_and_abstract.search = compact(search),
        group_by = "publication_year",
        paging = "cursor",
        verbose = FALSE
    )$key

    #######
    #######
    # processed <- list.dirs(
    #     path = pages_dir,
    #     full.names = FALSE,
    #     recursive = FALSE
    # ) |>
    #     gsub(
    #         pattern = paste0("^pages_publication_year=", ""),
    #         replacement = ""
    #     )

    # interrupted <- list.files(
    #     path = pages_dir,
    #     pattern = "^next_page.rds",
    #     full.names = TRUE,
    #     recursive = TRUE
    # ) |>
    #     gsub(
    #         pattern = paste0("^", pages_dir, "/pages_publication_year=", ""),
    #         replacement = ""
    #     ) |>
    #     gsub(
    #         pattern = "/next_page.rds$",
    #         replacement = ""
    #     )

    # completed <- processed[!(processed %in% interrupted)]

    # years <- years[!(years %in% completed)]
    #######
    #######

    result <- pbmcapply::pbmclapply(
        sample(years),
        function(y) {
            message("\nGetting data for year ", y, " ...")
            output_path <- file.path(pages_dir, paste0(rds_dir, y))

            dir.create(
                path = output_path,
                showWarnings = FALSE,
                recursive = TRUE
            )

            data <- oa_query(
                title_and_abstract.search = compact(search),
                publication_year = y,
                options = list(
                    select = c("id", "doi", "authorships", "publication_year", "display_name", "abstract_inverted_index", "topics")
                ),
                verbose = FALSE
            ) |>
                IPBES.R::oa_request_IPBES(
                    count_only = FALSE,
                    output_path = output_path,
                    verbose = TRUE
                )
        },
        mc.cores = 8,
        mc.preschedule = FALSE
    ) |>
        unlist()

    invisible(result)
}

sts <- sapply(
    R1_st,
    function(st) {
        compact(paste0("(", st, ") AND (", R2_st, ")"))
    }
)


get_corpus_pages(
    search = sts[[1]],
    pages_dir = file.path(".", "data", "pages"),
    rds_dir = "pages_1_publication_year="
)


get_corpus_pages(
    search = sts[[2]],
    pages_dir = file.path(".", "data", "pages"),
    rds_dir = "pages_2_publication_year="
)


toc()
```

The fields `author` and `topics` are serialized in the arrow database and need to be unserialized by using `unserialize_arrow()` on a dataset containing the two columns.

```{r}
#| label: convert_tca_corpus_arrow
#| eval: false
tic()

pages_dir <- file.path(".", "data", "pages")
arrow_dir <- file.path(".", "data", "corpus")


years <- list.dirs(
    path = pages_dir,
    full.names = TRUE,
    recursive = FALSE
) |>
    strsplit(
        split = "="
    ) |>
    sapply(
        FUN = function(x) {
            x[2]
        }
    ) |>
    unique() |>
    sort()

# years_done <- list.dirs(
#     path = arrow_dir,
#     full.names = TRUE,
#     recursive = FALSE
# )

# years <- years[
#     (
#         gsub(
#             x = years,
#             pattern = paste0("^", pages_dir, "/pages_publication_year="),
#             replacement = ""
#         ) # %in% gsub(
#         #     x = years_done,
#         #     pattern = paste0("^", arrow_dir, "/publication_year="),
#         #     replacement = ""
#         # )
#     )
# ]

result <- pbapply::pblapply(
    sample(years),
    function(year) {
        message("\n     Processing year ", year, " ...\n")
        pages <- c(
            list.files(
                path = pages_dir,
                pattern = paste0("\\.rds$"),
                full.names = TRUE,
                recursive = TRUE
            ) |>
                grep(
                    pattern = year,
                    value = TRUE
                ) |>
                grep(
                    pattern = ".*publication.*",
                    value = TRUE
                )
        )
        data <- parallel::mclapply(
            pages,
            function(page) {
                # message("Processing ", page, " ...")
                p <- readRDS(file.path(page))$results
                if (length(p) == 0) {
                    p <- NULL
                } else {
                    p <- openalexR::works2df(p, verbose = FALSE)
                    p$author_abbr <- IPBES.R::abbreviate_authors(p)
                }
                return(p)
            },
            mc.cores = 1 # params$mc.cores
        ) |>
            do.call(what = dplyr::bind_rows) |>
            distinct(id, .keep_all = TRUE)

        saveRDS(
            data,
            file = file.path(pages_dir, paste0(year, ".rds"))
        )

        data <- serialize_arrow(data)

        arrow::write_dataset(
            data,
            path = arrow_dir,
            partitioning = "publication_year",
            format = "parquet",
            existing_data_behavior = "overwrite"
        )
    }
)

arrow_dir <- file.path(".", "data", "corpus_tca")

read_corpus("data/corpus") |>
    dplyr::collect() |>
    dplyr::filter(id %in% readRDS("data/ids_subs_tca.rds")) |>
    arrow::write_dataset(
        path = arrow_dir,
        partitioning = "publication_year",
        format = "parquet",
        existing_data_behavior = "overwrite"
    )

toc()
```


## Identify Works in Subsidies Corpus AND TCA corpus

```{r}
#| label: get_ids_subs_tca
#|

ids_subsidies <- read_corpus(file.path("data", "corpus")) |>
    dplyr::select(id) |>
    collect() |>
    unlist()

ids_tca <- read_corpus(file.path("..", "IPBES_TCA_Corpus", "data", "corpus")) |>
    dplyr::select(id) |>
    collect() |>
    unlist()

ids_subs_tca <- ids_tca[ids_tca %in% ids_subsidies]

saveRDS(ids_subs_tca, file = file.path("data", "ids_subs_tca.rds"))
```



## Export data for sentiment analysis

```{r}
#| label: export_sentiment_analysis
#| eval: false
#|

read_corpus("data/corpus") |>
    dplyr::select(id, publication_year, ab) |>
    dplyr::collect() |>
    write.table(file = "sent_analysis_subsidies.txt")

read_corpus("data/corpus_tca") |>
    dplyr::select(id, publication_year, ab) |>
    dplyr::collect() |>
    write.table(file = "sent_analysis_subsidies_tca.txt")
```

## Export 50 random papers for manual analysis

```{r}
#| label: export_random_50
#| eval: false
#|

set.seed(13)

read_corpus("data/corpus") |>
    dplyr::select(id, doi, author_abbr, display_name, ab) |>
    dplyr::rename(abstract = ab, title = display_name) |>
    dplyr::slice_sample(n = 50) |>
    dplyr::collect() |>
    writexl::write_xlsx(path = "random_50_subsidies.xlsx")

set.seed(14)

read_corpus("data/corpus_tca") |>
    dplyr::select(id, doi, author_abbr, display_name, ab) |>
    dplyr::rename(abstract = ab, title = display_name) |>
    dplyr::slice_sample(n = 50) |>
    dplyr::collect() |>
    writexl::write_xlsx(path = "random_50_subsidies_in_tca.xlsx")
```



# Results

### Number of hits

The number of hits are hits of the terms of the whole of the OpenAlex corpus. Due to methodological issues, the number of `R1 AND R2` are overestimates and contain some double counting.

- **`R1`** in OpenAlex: `r formatC(R1_count, format="f", big.mark=",", digits=0)` hits
- **`R2`** in OpenAlex: `r formatC(R2_count, format="f", big.mark=",", digits=0)` hits
- **`R1 AND R2`**: in OpenAlex `r formatC(R1_R2_count, format="f", big.mark=",", digits=0)` hits
- **`R1 AND R2`** in TCA corpus: `r formatC(length(readRDS("data/ids_subs_tca.rds")), format="f", big.mark=",", digits=0)` hits


### Manual review 50 paper

The file contains the `id`, `doi`, `author_abbr` and `abstract` of the papers. Two samples were generated:

- works in the subsidies corpus which can be downloded [here](random_50_subsidies.xlsx){target=_blank}. 
- works in the subsidies corpus AND in the TCA corpus which can be downloded [here](random_50_subsidies_tca.xlsx){target=_blank}. 


### Sentiment analysis

Two csv containing the `id`, `publication_year` and `ab' (abstract) were extracted:

- works in the subsidies corpus which can be downloded [here](sent_analysis_subsidies.txt){target=_blank}. 
- works in the subsidies corpus AND in the TCA corpus which can be downloded [here](sent_analysis_subsidies_tca.txt){target=_blank}. 

### Publications over time

```{r}

read_corpus(file.path("data", "corpus_tca")) |>
    dplyr::select(publication_year) |>
    arrange(publication_year) |>
    dplyr::collect() |>
    table() |>
    as.data.frame() |>
    mutate(
        p = Freq / sum(Freq),
        p_cum = cumsum(p)
    ) |>
    ggplot() +
    geom_bar(aes(x = as.factor(publication_year), y = p), stat = "identity") +
    labs(
        title = "Publications over time",
        x = "Year",
        y = "Number of publications"
    ) +
    scale_x_discrete(breaks = seq(1970, 2020, 5)) +
    theme_minimal()
```

### Map first author affiliation
 Distribution of the first author affiliation countries
 
```{r}
dat <- read_corpus(file.path("data", "corpus_tca")) |>
    dplyr::select(author) |>
    collect()
sapply(
    dat$author,
    function(x) {
        auth <- ifelse(
            is.na(x),
            NA,
            purrr::map(x, ~ .x |>
                base64enc::base64decode() |>
                unserialize())
        )
        if (is.na(auth)) {
            return(NULL)
        } else {
            return(auth[[1]]$institution_country_code[[1]])
        }
    }
) |>
    unlist() |>
    table() |>
    as.data.frame() |>
    rename(
        iso2c = Var1,
        n = Freq
    ) |>
    mutate(
        log_n = log(n + 1)
    ) |>
    IPBES.R::map_country_codes(
        values = "log_n"
    )
```