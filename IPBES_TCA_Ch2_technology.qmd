---
title: 'Chapter 2 Technology Visions - Technical Background Report'
subtitle: 'IPBES Transformative Change Assessment'
author:
  - name: 
        family: Krug
        given: Rainer M.    
        id: rmk
    orcid: 0000-0002-7490-0066
    email: Rainer@Krugs.de
    roles: [author, editor]
  - name: 
        family: Villasante
        given: Sebastian    
    id: sv
    orcid: 0000-0001-6296-4479
    roles: [author] 
#   - name: 
#         family: Lambertucci
#         given: Sergio    
#     id: ls
#     orcid: 0000-0002-2624-2185
#     roles: [author] 
  - name: 
        family: Reyes Garcia
        given: Victoria    
    id: rgv
    orcid: 
    roles: [author]
  - name:
        family: Shannon
        given: Lynne
    id: ls
    orcid: 
    roles: [author]

#   - name: 
#         family: Agrawal
#         given: Arun    
#     id: aa
#     orcid: 0000-0001-6796-2958
#     roles: [author] 
#   - name: 
#         family: O'Brien
#         given: Karen    
#     id: ok
#     orcid: 
#     roles: [author] 
#   - name: 
#         family: Garibaldi
#         given: Lucas    
#     id: 
#     orcid: 0000-0002-7032-8330
#     roles: [author] 
license: "CC BY"
copyright: 
  holder: No idea
  year: 2024
citation: 
  type: report
  container-title: 'IPBES Transformative Change Assessment'
  doi: 10.5281/zenodo.11389482
doi: 10.5281/zenodo.11389482
version: 0.0.1

format:
    html:
        toc: true
        toc-depth: 4
        toc_expand: true
        embed-resources: true
        code-fold: true
        code-summary: 'Show the code'
        grid:
            sidebar-width: 0px
            body-width: 4000px
            margin-width: 200px
            gutter-width: 1.5rem      

params:
    gdm_dir: !expr file.path("ch2_technology", "data", "gdm_dir")
    #
    st_vision: !expr paste(readLines(file.path("ch2_technology", "input", "search_terms", "vision.txt")), collapse = " ")
    st_technology: !expr paste(readLines(file.path("ch2_technology", "input", "search_terms", "technology.txt")), collapse = " ")
    st_marine: !expr paste(readLines(file.path("ch2_technology", "input", "search_terms", "marine.txt")), collapse = " ")
    #
    fn_sentiment_results: !expr file.path("ch2_technology", "input", "sentiment_results.rds")
    #
    st_case: !expr paste(readLines(file.path("tca_corpus", "input", "search terms", "case.txt")), collapse = " ")
    #
    pages_dir: !expr file.path(".", "ch2_technology", "data", "pages_complete")
    #
    corpus_complete_dir: !expr file.path(".", "ch2_technology", "data", "corpus_complete")
    corpus_dir: !expr file.path(".", "ch2_technology", "data", "corpus")
    corpus_authors_dir: !expr file.path("ch2_technology", "data", "corpus_authors")
    corpus_topics_dir: !expr file.path("ch2_technology", "data", "corpus_topics")
    #
    corpus_tca_dir: !expr file.path("ch2_technology", "data", "corpus")
    corpus_cases_dir: !expr file.path("tca_corpus", "data", "corpus_cases")
    #
    fn_count: !expr file.path("ch2_technology", "data", "oa_count.rds")
    fn_ids_tech_in_tca: !expr file.path("ch2_technology", "data", "ids_tech_in_tca.rds")
    fn_sent_analysis_parquet: !expr file.path("ch2_technology", "data", "sent_analysis_technology.parquet")
    fn_random_sample_250: !expr file.path("ch2_technology", "data", "random_250_technology_in_tca.xlsx")
    fn_publications_over_time: !expr file.path("ch2_technology", "data", "publications_over_time.rds")
    #
    fn_sentiment_spatial_data: !expr file.path("ch2_technology", "data", "sentiment_spatial_data.rds")
    fn_sentiment_temporal_data: !expr file.path("ch2_technology", "data", "sentiment_temporal_data.rds")
    fn_sentiment_marine_temporal_data: !expr file.path("ch2_technology", "data", "sentiment_marine_temporal_data.rds")
    fn_marine_sentiment:  !expr file.path("ch2_technology", "data", "sentiment_marine_export.xlsx")
    fn_marine_sentiment_all:  !expr file.path("ch2_technology", "data", "sentiment_marine_all_export.xlsx")
    #
    fig_publications_over_time: !expr file.path("ch2_technology", "figures", "publications_over_time")
    fig_publications_over_time_yearly: !expr file.path("ch2_technology", "figures", "publications_over_time_yearly")
    #
    temporal_from: 1900
    min_count_sentiment_timeseries: 10
---

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.11389482.svg)](https://doi.org/10.5281/zenodo.11389482)
[![GitHub release](https://img.shields.io/github/release/IPBES-Data/IPBES_TCA_Corpus.svg)](https://github.com/IPBES-Data/IPBES_TCA_Corpus/releases/latest)
[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)

Part of the Data Management Report [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10251349.svg)](https://doi.org/10.5281/zenodo.10251349)

# Disclaimer
This document is part of the Thematic assessment of the underlying causes of biodiversity loss, determinants of transformative change and options for achieving the 2050 vision for biodiversity. It is a technical background document to the [Data Management Report for the Transformative Change Assessment Corpus](https://doi.org/10.5281/zenodo.10251349). This document is a working document which is not approved by the plenary. For statements pelase see the approved version of the assessment. This document has the sole purpose to document the workflows used to obtain some statements, figures and maps, document the source of the data and make the process reproducible.

# Contributors
<!-- ## Assessment Experts

- Shannon, Lynne
- Liao, Chuan
- Zinngrebe, Yves [![ORCID](./images/ORCID-iD_icon-16x16.png)](https://orcid.org/0000-0003-1731-2222) 
- Frantzeskaki, Niki [![ORCID](./images/ORCID-iD_icon-16x16.png)](https://orcid.org/0000-0002-6983-448X)
- Gurung, Janita [![ORCID](./images/ORCID-iD_icon-16x16.png)](https://orcid.org/0000-0003-1053-4412)
- Leventon, Julia [![ORCID](./images/ORCID-iD_icon-16x16.png)](https://orcid.org/0000-0002-2447-8522) 
- Wickson, Fern
- Biggs, Oonsie
- Bennett, Elena
- CalderÃ³n-Contreras, Rafael [![ORCID](./images/ORCID-iD_icon-16x16.png)](https://orcid.org/0000-0001-8709-4502)
- Gosnell, Hannah
- Cantu Fernandez, Mariana
- Carr, Edward [![ORCID](./images/ORCID-iD_icon-16x16.png)](https://orcid.org/0000-0001-7784-471X)
- Benessaiah, Karina [![ORCID](./images/ORCID-iD_icon-16x16.png)](https://orcid.org/0000-0001-9959-554X) -->

## Data and Knowledge tsu

- Niamir, Aidin [![ORCID](./images/ORCID-iD_icon-16x16.png)](https://orcid.org/0000-0003-4511-3407)
- Gudde, Renske  [![ORCID](./images/ORCID-iD_icon-16x16.png)](https://orcid.org/0000-0003-4727-1011)


```{r}
#| label: setup
#| include: false

if (!exists("params")) {
    params <- rmarkdown::yaml_front_matter("IPBES_TCA_Ch2_technology.qmd")$params
}

build <- as.integer(readLines(file.path("ch2_technology", "buildNo")))
build <- build + 1
writeLines(as.character(build), file.path("ch2_technology", "buildNo"))

knitr::opts_chunk$set(message = NA)

library(openalexR)
library(arrow)
library(duckdb)
library(DBI)
library(dplyr)
library(ggplot2)
library(ggridges)
library(IPBES.R)
library(tictoc)

#####

get_count <- !file.exists(params$fn_count)
if (get_count) {
    oa_count <- list(
        timestamp = Sys.time()
    )
} else {
    oa_count <- readRDS(params$fn_count)
}

#####

source(file.path("ch2_technology", "R", "functions.R"))

```



# Working Title
IPBES_TCA_Ch2_technology

# Code repo

[Github - private](https://github.com/IPBES-Data/IPBES_TCA_Ch2_technology)

# Build No: `r build`

The BuidNo is automatically increased by one each time the report is rendered. It is used to indicate different renderings when the version stays the same.

## Introduction

All searches are done on all works in OpenAlex. The search in the TCA Corpus is not possibly at the moment, but we are working on it.


# Methods


## Download of Data Files from Zenodo

The `data` folder is also available in a separate deposit at [10.5281/zenodo.11389148](https://doi.org/10.5281/zenodo.11389207).

To guarantee reproducibility, it will be downloaded and extracted when the folder `ch2_technology/data` does not exist

All code to re-generate the data is included but might take a long time to run and produce different numbers as OpenAlex is updated continously.

Disable this block and delete all content in the folder `ch2_technology/data` to re-generate the data. The folder `ch2_technology/data` has to exist.

This code will only work after the approval of the assessment by the plenary as the repository will remain confidential before.

```{r}
#| label: download_data
#| eval: true 

dn <- file.path("ch2_technology", "data")
if (!dir.exists(dn)) {
    dir.create(dn)
    url <- paste0("https://zenodo.org/record/", "11389207", "/files/data.zip")
    destfile <- tempfile(fileext = ".zip")
    download.file(url, destfile)

    unzip(destfile, exdir = "data")
}
```

## OpenAlex Complete

```{r}
#| label: get_oa_count
#|

if (get_count) {
    oa_count$oa <- openalexR::oa_fetch(
        entity = "works",
        search = "",
        output = "list",
        count_only = TRUE,
        verbose = TRUE
    )$count
}
```

```{r}
#| label: get_oa_years
#|

if (get_count) {
    oa_count$oa_years <- openalexR::oa_fetch(
        group_by = "publication_year",
        output = "dataframe",
        verbose = TRUE
    ) |>
        dplyr::mutate(
            publication_year = as.integer(as.character(key_display_name)),
            key = NULL,
            key_display_name = NULL,
            p = count / sum(count)
        ) |>
        dplyr::arrange(publication_year) |>
        dplyr::mutate(
            p_cum = cumsum(p)
        ) |>
        dplyr::select(
            publication_year, 
            everything()
        )
}
```

## Vision

The search terms is [vision](ch2_technology/input/vision.txt){target=_blank}
Open Alex search.

```{r}
#| label: get_vision_count
#|

if (get_count) {
    oa_count$vision <- openalexR::oa_fetch(
        title_and_abstract.search = params$st_vision,
        count_only = TRUE,
        output = "list",
        verbose = TRUE
    )$count
}

```
```{r}
#| label: get_vision_years
#|

if (get_count) {
    oa_count$vision_years <- openalexR::oa_fetch(
        title_and_abstract.search = params$st_vision,
        group_by = "publication_year",
        output = "dataframe",
        verbose = TRUE
    ) |>
        dplyr::mutate(
            publication_year = as.integer(as.character(key_display_name)),
            key = NULL,
            key_display_name = NULL,
            p = count / sum(count)
        ) |>
        dplyr::arrange(publication_year) |>
        dplyr::mutate(
            p_cum = cumsum(p)
        ) |>
        dplyr::select(
            publication_year,
            everything()
        )
}
```



## Technology

The search terms is [technology](ch2_technology/input/technology.txt){target=_blank}
Open Alex search.



```{r}
#| label: get_technology_count
#|

if (get_count) {
    oa_count$technology <- openalexR::oa_fetch(
        title_and_abstract.search = compact(params$st_technology),
        count_only = TRUE,
        output = "list",
        verbose = TRUE
    )$count
}
```


```{r}
#| label: get_technology_years
#|

if (get_count) {
    oa_count$technology_years <- openalexR::oa_fetch(
        title_and_abstract.search = compact(params$st_technology),
        group_by = "publication_year",
        output = "dataframe",
        verbose = TRUE
    ) |>
        dplyr::mutate(
            publication_year = as.integer(as.character(key_display_name)),
            key = NULL,
            key_display_name = NULL,
            p = count / sum(count)
        ) |>
        dplyr::arrange(publication_year) |>
        dplyr::mutate(
            p_cum = cumsum(p)
        ) |>
        dplyr::select(
            publication_year,
            everything()
        )
}
```


## Vision AND technology
Open Alex search.

The search term is [vision](ch2_technology/input/vision.txt){target=_blank} AND [technology](ch2_technology/input/technology.txt){target=_blank}



```{r}
#| label: get_vision_technology_count
#|

if (get_count) {
    oa_count$vision_technology <- openalexR::oa_fetch(
        title_and_abstract.search = compact(paste0("(", params$st_vision, ") AND (", params$st_technology, ")")),
        output = "list",
        count_only = TRUE,
        verbose = TRUE
    )$count
}
```

```{r}
#| label: get_vision_technology_years
#|

if (get_count) {
    oa_count$vision_technology_years <- openalexR::oa_fetch(
        title_and_abstract.search = compact(paste0("(", params$st_vision, ") AND (", params$st_technology, ")")),
        group_by = "publication_year",
        output = "dataframe",
        verbose = TRUE
    ) |>
        dplyr::mutate(
            publication_year = as.integer(as.character(key_display_name)),
            key = NULL,
            key_display_name = NULL,
            p = count / sum(count)
        ) |>
        dplyr::arrange(publication_year) |>
        dplyr::mutate(
            p_cum = cumsum(p)
        ) |>
        dplyr::select(
            publication_year,
            everything()
        )
}
```


```{r}
#| label: get_vision_technology_subfield_years
#|
if (get_count) {
    oa_count$vision_technology_subfields <- openalexR::oa_query(
        title_and_abstract.search = compact(paste0("(", params$st_vision, ") AND (", params$st_technology, ")")),
        group_by = "primary_topic.subfield.id",
        verbose = TRUE
    ) |>
        openalexR::oa_request() |>
        dplyr::bind_rows() |>
        dplyr::arrange(key)

    ## clean up missing or wrong vision_technology_subfields$key_display_name
    need_cleaning <- is.na(oa_count$vision_technology_subfields$key_display_name) |
        !is.na(as.numeric(oa_count$vision_technology_subfields$key_display_name))

    fine <- !need_cleaning

    oa_count$vision_technology_subfields <- oa_count$vision_technology_subfields |>
        dplyr::filter(fine) |>
        dplyr::select(key, key_display_name) |>
        dplyr::distinct() |>
        merge(y = oa_count$vision_technology_subfields[need_cleaning, -2], by = "key") |>
        dplyr::bind_rows(oa_count$vision_technology_subfields[fine, ]) |>
        dplyr::group_by(key, key_display_name) |>
        dplyr::summarize(count = sum(count))
}
```



## Vision AND technology AND marine
Open Alex search.

The search term is [vision](ch2_technology/input/vision.txt){target=_blank} AND [technology](ch2_technology/input/technology.txt){target=_blank}



```{r}
#| label: get_vision_technology_marine_count
#|

if (get_count) {
    oa_count$vision_technology_marine <- openalexR::oa_fetch(
        title_and_abstract.search = compact(paste0("(", params$st_vision, ") AND (", params$st_technology, ") AND (", params$st_marine, ")")),
        output = "list",
        count_only = TRUE,
        verbose = TRUE
    )$count
}
```

```{r}
#| label: get_vision_technology_marine_years
#|

if (get_count) {
    oa_count$vision_technology_marine_years <- openalexR::oa_fetch(
        title_and_abstract.search = compact(paste0("(", params$st_vision, ") AND (", params$st_technology, ") AND (", params$st_marine, ")")),
        group_by = "publication_year",
        output = "dataframe",
        verbose = TRUE
    ) |>
        dplyr::mutate(
            publication_year = as.integer(as.character(key_display_name)),
            key = NULL,
            key_display_name = NULL,
            p = count / sum(count)
        ) |>
        dplyr::arrange(publication_year) |>
        dplyr::mutate(
            p_cum = cumsum(p)
        ) |>
        dplyr::select(
            publication_year,
            everything()
        )
}
```



```{r}
#| label: get_vision_technology_marine_ids
#|
if (get_count) {
    oa_count$vision_technology_marine_ids <- openalexR::oa_fetch(
        title_and_abstract.search = compact(paste0("(", params$st_vision, ") AND (", params$st_technology, ") AND (", params$st_marine, ")")),
        verbose = TRUE,
        output = "list",
        options = list(
            select = "id"
        )
    ) |>
        unlist() |>
        unique()
}
```


Open Alex search.

The search term is [vision](ch2_technology/input/vision.txt){target=_blank} AND [technology](ch2_technology/input/technology.txt){target=_blank}

### Count of Publications

```{r}
#| label: get_vision_technology_case_count
#| eval: false

# To long search string
if (get_count) {
    oa_count$vision_technology_case <- openalexR::oa_fetch(
        title_and_abstract.search = compact(paste0("(", params$st_vision, ") AND (", params$st_technology, ") AND (", params$st_case, ")")),
        output = "list",
        count_only = TRUE,
        verbose = TRUE
    )$count
}
```


```{r}
#| label: get_vision_technology_case_years
#|

if (get_count) {
    oa_count$vision_technology_case_years <- openalexR::oa_fetch(
        title_and_abstract.search = compact(paste0("(", params$st_vision, ") AND (", params$st_technology, ") AND (", params$st_case, ")")),
        group_by = "publication_year",
        output = "dataframe",
        verbose = TRUE
    ) |>
        dplyr::mutate(
            publication_year = as.integer(as.character(key_display_name)),
            key = NULL,
            key_display_name = NULL,
            p = count / sum(count)
        ) |>
        dplyr::arrange(publication_year) |>
        dplyr::mutate(
            p_cum = cumsum(p)
        ) |>
        dplyr::select(
            publication_year,
            everything()
        )
}
```


## Save the counts as `r params$fn_count`

```{r}
#| label: save_oa_count
#| 

if (get_count) {
    saveRDS(oa_count, params$fn_count)
}
```

## Download `technology AND vision` Corpus from OpenAlex

The corpus download will be stored in `ch2_technology/pages` and the parquet database in `ch2_technology/data/corpus_complete`. This one will be filtered with the TCA / Global Corpus and get the final name `ch2_technology/data/corpus`.

This is not on github!

The corpus can be read by running `corpus_read("ch2_technology/data/corpus")` which opens the database so that then it can be fed into a `dplyr` pipeline. After most `dplyr` functions, the actual data needs to be collected via `collect()`.

Only then is the actual data read!

Needs to be enabled by setting `eval: true` in the code block below.

### Download Complete Corpus

```{r}
#| label: get_technology_corpus
#| eval: false
#|

tic()

IPBES.R::corpus_download(
    pages_dir = file.path(".", "ch2_technology", "data", "pages"),
    title_and_abstract_search = compact(paste0("(", params$st_vision, ") AND (", params$st_technology, ")")),
    continue = TRUE,
    delete_pages_dir = FALSE,
    set_size = 2000,
    dry_run = TRUE,
    verbose = TRUE,
    mc_cores = 6
)

toc()
```

### Convert Technology Corpus to Parquet datase

The fields `author` and `topics` are serialized in the parquet database and need to be unserialized by using `unserialize_arrow()` on a dataset containing the two columns.

```{r}
#| label: convert_tca_corpus_parquet
#| eval: false
#| 

tic()

IPBES.R::corpus_pages_to_arrow(
    pages_dir = params$pages_dir,
    arrow_dir = params$corpus_complete_dir,
    continue = TRUE,
    delete_arrow_dir = FALSE,
    dry_run = FALSE,
    verbose = TRUE,
    mc_cores = 3
)
toc()
```

### Filter Corpus with TCA Corpus
```{r}
#| label: filter_corpus_with_tca
#| eval: false
#|

tic()

if (!file.exists(params$fn_ids_tech_in_tca)) {
    ids_technology <- IPBES.R::corpus_read(params$corpus_complete) |>
        dplyr::select(id) |>
        collect() |>
        unlist()

    ids_tca <- read_corpus(file.path("..", "IPBES_TCA_Corpus", "ch2_technology", "data", "tca_corpus", "corpus")) |>
        dplyr::select(id) |>
        collect() |>
        unlist()

    fn_ids_tech_in_tca <- ids_technology[ids_technology %in% ids_tca]

    rm(ids_technology, ids_tca)

    saveRDS(ids_tech_in_tca, params$fn_ids_tech_in_tca)

    IPBES.R::corpus_filter_ids(
        arrow_dir = params$corpus_complete_dir,
        arrow_filter_dir = params$corpus_dir,
        filter_ids = ids_tech_in_tca
    )

    rm(ids_tech_in_tca)
}

toc()
```



Check the number of dulicates before running this next block, and then verify the new corpus afterwards. RUN ONLY MANUALY!

```{r}
#| label: fix_duplicate_ids_TEMPORARY
#| eval: false
#|

ONLY RUN MANUALLY!!!!!!!!!!!!!!!!!!!!!!!

(read_corpus(params$corpus_dir) |> group_by(id) |> summarize(n = n()) |> filter(n > 1) |> collect() |> nrow()) / (corpus_read(params$corpus_dir) |> nrow())

years <- IPBES.R::corpus_read(params$corpus_dir) |>
    distinct(publication_year) |>
    collect() |>
    unlist() |>
    as.vector() |>
    sort()

lapply(
    years,
    function(y) {
        message("\nProcessing year: ", y)
        tic()
        dataset <- IPBES.R::corpus_read(params$corpus_dir) |>
            dplyr::filter(publication_year == y) |>
            dplyr::collect() |>
            group_by(id) |>
            slice_max(
                publication_year,
                n = 1,
                with_ties = FALSE,
                na_rm = TRUE
            )
        # unlink(
        #     file.path(params$corpus_dir, paste0("publication_year=", y)),
        #     recursive = TRUE,
        #     force = TRUE
        # )
        arrow::write_dataset(
            dataset = dataset,
            path = paste0(params$corpus_dir, "_deduplicated"),
            partitioning = c("publication_year", "set"),
            format = "parquet",
            existing_data_behavior = "overwrite"
        )
        toc()
    }
)

(read_corpus("./ch2_technology/data/corpus_deduplicated") |> 
    group_by(id) |> 
    summarize(n = n()) |> 
    filter(n > 1) |> 
    collect() |> 
    nrow()) / (corpus_read("./ch2_technology/data/corpus_deduplicated") |> nrow())

rename corpora now 

NOW IF EVERYTHING IS OK, DELETE THE OLD CORPUS AND RENAME THE NEW ONE
```


## Sentiment Analysis

The Sentiment Analysis has been implemented by [Maral Dadvar](mailto://dadvar.maral@gmail.com)

The sentiment analys is based on the abstracts of the works. As not all works has abstracts, the number of datapoints in the sentiment analysis is smaller then the number of works. The sentiment analysis was implemented using the [Python NLTK package](https://www.nltk.org/), and [VADER](https://www.nltk.org/_modules/nltk/sentiment/vader.html) which is an NLTK module that provides sentiment scores based on the words used. Vader is a pre-trained, rule-based sentiment analysis model in which the terms are generally labeled as per their semantic orientation as either positive or negative. The main advantage/reason for using this model was that it doesn't require a labelled training dataset.

It returns a positive, negative, neutral and compound score. The compound score is a composite score that summarizes the overall sentiment of the text, where scores close to 1 indicate a positive sentiment, scores close to -1 indicate a negative sentiment, and scores close to 0 indicate a neutral sentiment. The other three scores show the percentage of each of the sentiments in the text.

The input is in parquet format, containing 124,518 entries, including publicationsâ abstracts, OpenAlex id, and publication year. 

The output of the model is 4 scores: compound, negative, neutral, and positive.
It is also in parquet format containing 124,518 entries, including  OpenAlex id, publication year, compound score, negative score, neutral score, and positive score.


```{python}
#| eval: false
#| 

import pandas as pd
import pyarrow.parquet as pq
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import csv
from multiprocessing import Pool, cpu_count
import numpy as np  

# Function to perform sentiment analysis on a chunk of rows
def analyze_sentiment(chunk):
    sid = SentimentIntensityAnalyzer()
    results = []
    for _, row in chunk.iterrows():
        if isinstance(row['ab'], str):  # Check if the 'ab' column contains a string
            dic = sid.polarity_scores(row['ab'])
            results.append([row['id'], row['publication_year'], dic['compound'], dic['neg'], dic['neu'], dic['pos']])
    return results

if __name__ == '__main__':
    # Read Parquet file
    table = pq.read_table('C:\\Users\\user\\Desktop\\IPBES\\SentimentAnalysis\\Sentimet_Analysis_Sebastian\\sent_analysis_technology.parquet')
    df = table.to_pandas()

    # Convert necessary columns to appropriate data types
    df['ab'] = df['ab'].astype(str)  

    # Determine the number of processes to use (up to the number of CPU cores)
    num_processes = min(cpu_count(), len(df))

    # Split DataFrame into chunks for parallel processing
    chunks = np.array_split(df, num_processes)

    with open('output_multi.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['id','date','compound','neg','neu','pos'])

        # Create a pool of worker processes
        with Pool(processes=num_processes) as pool:
            results = pool.map(analyze_sentiment, chunks)
            for result_chunk in results:
                for result_row in result_chunk:
                    writer.writerow(result_row)

    print("Sentiment analysis completed and results written to output_02_multi.csv")

```



## Export data for sentiment analysis


```{r}
#| label: export_sentiment_analysis
#|

if (!file.exists(params$fn_sent_analysis_parquet)) {
    corpus_read(params$corpus_dir) |>
        dplyr::select(id, publication_year, ab) |>
        arrow::write_parquet(params$fn_sent_analysis_parquet)
}

```

## Extract 250 random papers from `technology AND vision` in TCA Corpus
```{r}
#| label: extract_random_papers
#|

if (!file.exists(params$fn_random_sample_250)) {
    set.seed(14)
    read_corpus(params$corpus_dir) |>
        dplyr::select(id, doi, author_abbr, display_name, ab) |>
        dplyr::rename(abstract = ab, title = display_name) |>
        dplyr::collect() |>
        dplyr::slice_sample(n = 250) |>
        dplyr::mutate(
            abstract = substr(abstract, start = 1, stop = 5000)
        ) |>
        writexl::write_xlsx(path = params$fn_random_sample_250)
}
```


## Extract Authors

```{r}
#| label: authors_technology_corpus_data
#|

if (!dir.exists(params$corpus_authors_dir)) {
    con <- duckdb::dbConnect(duckdb::duckdb(), read_only = FALSE)

    corpus_read(params$corpus_dir) |>
        arrow::to_duckdb(table_name = "corpus", con = con) |>
        invisible()


    paste0(
        "CREATE VIEW corpus_unnest AS ",
        "SELECT  ",
        "corpus.id AS work_id,  ",
        "corpus.publication_year AS publication_year,  ",
        "UNNEST(author).au_id AS au_id,  ",
        "UNNEST(author).au_display_name AS au_display_name, ",
        "UNNEST(author).au_orcid AS au_orcid,  ",
        "UNNEST(author).author_position AS author_position,  ",
        "UNNEST(author).is_corresponding AS is_corresponding,  ",
        "UNNEST(author).au_affiliation_raw AS au_affiliation_raw,  ",
        "UNNEST(author).institution_id AS institution_id,  ",
        "UNNEST(author).institution_display_name AS institution_display_name,  ",
        "UNNEST(author).institution_ror AS institution_ror,  ",
        "UNNEST(author).institution_country_code AS institution_country_code,  ",
        "UNNEST(author).institution_type AS institution_type,  ",
        "UNNEST(author).institution_lineage AS institution_lineage  ",
        "FROM  ",
        "corpus "
    ) |> DBI::dbExecute(conn = con)

    paste0(
        "COPY ( ",
        "SELECT * FROM corpus_unnest ",
        ") TO '", params$corpus_authors_dir, "' ",
        "(FORMAT PARQUET, COMPRESSION 'SNAPPY', PARTITION_BY 'publication_year')"
    ) |>
        dbExecute(conn = con)

    duckdb::dbDisconnect(con, shutdown = TRUE)
}
```

## Publications over time
```{r}
#| label: publications_over_time_data
## | |

if (!file.exists(params$fn_publications_over_time)) {
    cases_ids <- corpus_read(params$corpus_cases_dir) |>
        dplyr::select(id, publication_year) |>
        collect() |>
        unlist()

    tech_cases <- corpus_read(params$corpus_dir) |>
        dplyr::select(publication_year, id) |>
        dplyr::filter(id %in% cases_ids) |>
        dplyr::group_by(publication_year) |>
        dplyr::select(publication_year) |>
        collect() |>
        table() |>
        as.data.frame() |>
        dplyr::mutate(
            publication_year = as.integer(as.character(publication_year)),
            p = Freq / sum(Freq),
            p_cum = cumsum(p)
        ) |>
        dplyr::rename(
            count_technology_vision_cases = Freq,
            p_count_technology_vision_cases = p,
            p_cum_count_technology_vision_cases = p_cum
        )


    tech_tca <- corpus_read(params$corpus_dir) |>
        dplyr::select(publication_year) |>
        dplyr::arrange(publication_year) |>
        dplyr::collect() |>
        table() |>
        as.data.frame() |>
        dplyr::mutate(
            publication_year = as.integer(as.character(publication_year)),
            p_tech_tca = Freq / sum(Freq),
            p_cum_tech_tca = cumsum(p_tech_tca)
        ) |>
        dplyr::rename(
            count_tech_tca = Freq
        )


    read_corpus(params$corpus_tca_dir) |>
        dplyr::select(publication_year) |>
        dplyr::arrange(publication_year) |>
        dplyr::collect() |>
        table() |>
        as.data.frame() |>
        dplyr::mutate(
            publication_year = as.integer(as.character(publication_year)),
            p = Freq / sum(Freq),
            p_cum = cumsum(p)
        ) |>
        dplyr::rename(
            count = Freq
        ) |>
        # oa complete
        dplyr::left_join(
            x = oa_count$oa_years,
            by = "publication_year",
            suffix = c("", "_tca")
        ) |>
        # oa vision
        dplyr::left_join(
            y = oa_count$vision_years,
            by = "publication_year",
            suffix = c("", "_vision")
        ) |>
        # oa technology
        dplyr::left_join(
            y = oa_count$technology_years,
            by = "publication_year",
            suffix = c("", "_technology")
        ) |>
        # oa vision technology
        dplyr::left_join(
            y = oa_count$vision_technology_years,
            by = "publication_year",
            suffix = c("", "_technology_vision")
        ) |>
        dplyr::rename(
            count_oa = count,
            p_oa = p,
            p_cum_oa = p_cum
        ) |>
        # technology in tca
        dplyr::left_join(
            y = tech_tca,
            by = "publication_year",
            suffix = c("", "_technology_vision_tca")
        ) |>
        # technology in cases
        dplyr::left_join(
            y = tech_cases,
            by = "publication_year",
            suffix = c("", "_technology_vision_tca_cases")
        ) |>
        saveRDS(file = params$fn_publications_over_time)
}
```

```{r}
#| label: publications_over_time_figure

if (length(list.files(path = dirname(params$fig_publications_over_time), pattern = basename(params$fig_publications_over_time)))< 6) {
    sec_axi_fact <- 0.5e-5


    figure <- readRDS(params$fn_publications_over_time) |>
        dplyr::filter(
            publication_year >= params$temporal_from,
            publication_year <= 2024
        ) |>
        ggplot2::ggplot() +
        #
        ggplot2::geom_bar(ggplot2::aes(x = publication_year, y = count_tca, fill = "Number of publications per year in TCA Corpus"), stat = "identity") +
        ggplot2::geom_line(ggplot2::aes(x = publication_year, y = p_cum_oa / sec_axi_fact, color = "Cumulative proportion OA Corpus"), linewidth = 1.5, linetype = "dotted") +
        ggplot2::geom_line(ggplot2::aes(x = publication_year, y = p_cum_tca / sec_axi_fact, color = "Cumulative proportion TCA Corpus"), linewidth = 1.5, linetype = "dotted") +
        ggplot2::geom_line(ggplot2::aes(x = publication_year, y = p_cum_vision / sec_axi_fact, color = "Cumulative proportion vision only corpus"), linewidth = 1.5, linetype = "dotted") +
        ggplot2::geom_line(ggplot2::aes(x = publication_year, y = p_cum_technology / sec_axi_fact, color = "Cumulative proportion technology only corpus"), linewidth = 1.5, linetype = "dotted") +
        ggplot2::geom_line(ggplot2::aes(x = publication_year, y = p_cum_technology_vision / sec_axi_fact, color = "Cumulative proportion Technology Corpus"), linewidth = 1.5, linetype = "solid") +
        ggplot2::geom_line(ggplot2::aes(x = publication_year, y = p_cum_count_technology_vision_cases / sec_axi_fact, color = "Cumulative proportion Technology Corpus min Cases Corpus"), linewidth = 1.5, linetype = "solid") +
        #
        ggplot2::scale_color_manual(
            values = c(
                "Cumulative proportion OA Corpus" = "#1f77b4",
                "Cumulative proportion TCA Corpus" = "black",
                "Cumulative proportion vision only corpus" = "#2ca02c",
                "Cumulative proportion technology only corpus" = "#d62728",
                "Cumulative proportion Technology Corpus" = "#9467bd",
                "Cumulative proportion Technology Corpus min Cases Corpus" = "#8c564b"
            )
        ) +
        ggplot2::scale_fill_manual(
            values = c("Number of publications per year in TCA Corpus" = "lightgrey")
        ) +
        #
        ggplot2::scale_x_continuous(breaks = seq(params$temporal_from, 2030, 10)) +
        ggplot2::scale_y_continuous(
            "Proportion of publications",
            sec.axis = ggplot2::sec_axis(~ . * sec_axi_fact, name = "Cumulative proportion") # divide by 100 to scale back the secondary axis
        ) +
        ggplot2::labs(
            title = "Publications over time",
            x = "Year",
            y = "Number of publications"
        ) +
        ggplot2::theme_minimal() +
        ggplot2::theme(
            axis.text.y.right = ggplot2::element_text(color = "red"),
            legend.position = "inside", # Move the legend to the top left position
            legend.justification = c(0.1, 0.9), # Justify the legend to the top left position
            legend.background = ggplot2::element_rect(fill = "white", color = "black") # Add a white background to the legend
        )
    figure

    ggplot2::ggsave(
        paste0(params$fig_publications_over_time, ".pdf"),
        width = 12,
        height = 12,
        figure
    )
    ggplot2::ggsave(
        paste0(params$fig_publications_over_time, ".svg"),
        width = 12,
        height = 12,
        figure
    )
    ggplot2::ggsave(
        paste0(params$fig_publications_over_time, ".png"),
        width = 12,
        height = 12,
        figure
    )

    rm(figure, sec_axi_fact)
}
```


```{r}
#| label: publications_over_time_yearly_figure
##| # | |
if (length(list.files(path = dirname(params$fig_publications_over_time), pattern = basename(params$fig_publications_over_time_yearly)))< 3) {
    data <- readRDS(params$fn_publications_over_time)
    max_count_technology_vision <- max(data$count_technology_vision, na.rm = TRUE)
    sec_axi_fact <- 1e3
    figure <- data |>
        dplyr::filter(
            publication_year >= params$temporal_from,
            publication_year <= 2024
        ) |>
        ggplot2::ggplot() +
        #
        ggplot2::geom_line(
            ggplot2::aes(
                x = publication_year,
                y = (count_tca / count_oa),
                color = "4. Proportion TCA Corpus of OA Corpus of OA Corpus",
                linetype = "Left Y Axis"
            ),
            linewidth = 1.5
        ) +
        ggplot2::geom_line(
            ggplot2::aes(
                x = publication_year,
                y = (count_vision / count_oa),
                color = "1. Proportion vision only corpus of OA Corpus",
                linetype = "Left Y Axis"
            ),
            linewidth = 1.5
        ) +
        ggplot2::geom_line(
            ggplot2::aes(
                x = publication_year,
                y = (count_technology / count_oa),
                color = "2. Proportion technology only corpus of OA Corpus",
                linetype = "Left Y Axis"
            ),
            linewidth = 1.5
        ) +
        ggplot2::geom_line(
            ggplot2::aes(
                x = publication_year,
                y = (count_technology_vision / count_oa),
                color = "3. Proportion Technology Corpus of OA Corpus",
                linetype = "Left Y Axis"
            ),
            linewidth = 1.5
        ) +
        ggplot2::geom_line(
            ggplot2::aes(
                x = publication_year,
                y = (count_technology_vision_cases / count_oa) * sec_axi_fact,
                color = "5. Proportion Technology Corpus in Cases Corpus of OA Corpus",
                linetype = "Right Y Axis"
            ),
            linewidth = 1.5
        ) +
        # #
        ggplot2::scale_color_manual(
            values = c(
                "1. Proportion vision only corpus of OA Corpus" = "#8c564b",
                "2. Proportion technology only corpus of OA Corpus" = "#2ca02c",
                "3. Proportion Technology Corpus of OA Corpus" = "#d62728",
                "4. Proportion TCA Corpus of OA Corpus of OA Corpus" = "#9467bd",
                "5. Proportion Technology Corpus in Cases Corpus of OA Corpus" = "black"
            ),
            labels = waiver()
        ) +
        #
        ggplot2::scale_linetype_manual(
            values = c(
                "Left Y Axis" = "dotted",
                "Right Y Axis" = "solid"
            ),
            labels = waiver()
        ) +
        #
        ggplot2::scale_x_continuous(breaks = seq(params$temporal_from, 2030, 10)) +
        ggplot2::scale_y_continuous(
            "Proportion of publications",
            sec.axis = ggplot2::sec_axis(~ . / sec_axi_fact, name = "Proportion of publications") # divide by 100 to scale back the secondary axis
        ) +
        ggplot2::labs(
            title = "Publications over time",
            x = "Year",
            y = "Number of publications"
        ) +
        ggplot2::theme_minimal() +
        ggplot2::theme(
            axis.text.y.right = ggplot2::element_text(color = "red"),
            legend.position = "inside", # Move the legend to the top left position
            legend.justification = c(0.1, 0.9), # Justify the legend to the top left position
            legend.background = ggplot2::element_rect(fill = "white", color = "black") # Add a white background to the legend
        ) +
        ggplot2::labs(color = "Proportion of OA Corpus")
    figure

    ggplot2::ggsave(
        paste0(params$fig_publications_over_time_yearly, ".pdf"),
        width = 12,
        height = 12,
        figure
    )
    ggplot2::ggsave(
        paste0(params$fig_publications_over_time_yearly, ".svg"),
        width = 12,
        height = 12,
        figure
    )
    ggplot2::ggsave(
        paste0(params$fig_publications_over_time_yearly, ".png"),
        width = 12,
        height = 12,
        figure
    )

    rm(figure, sec_axi_fact)
}
```

## Sentiment Analysis

```{r}
#| label: prepare_sent_results
#| eval: false
#| 

read.csv("ch2_technology/input/sent_analysis_technology_results_MD.csv") |>
    dplyr::group_by(id) |>
    dplyr::slice_min(
        order_by = row_number(), 
        n = 1
    ) |>
    saveRDS(file = params$fn_sentiment_results)

```

### Spatial


```{r}
#| label: sentiment_per_countries_data
#|

if (!file.exists(params$fn_sentiment_spatial_data)) {
    data <- corpus_read(params$corpus_authors_dir) |>
        dplyr::select(
            work_id,
            institution_country_code
        ) |>
        dplyr::filter(
            !is.na(institution_country_code)
        ) |>
        collect() |>
        mutate(
            iso3c = countrycode::countrycode(
                institution_country_code,
                origin = "iso2c",
                destination = "iso3c"
            ),
            institution_country_code = NULL
        ) |>
        dplyr::left_join(
            readRDS(params$fn_sentiment_results) |>
                dplyr::select(
                    work_id = id,
                    neg,
                    neu,
                    pos,
                    compound
                ) ,
            by = "work_id"
        ) |>
        dplyr::group_by(iso3c) |>
        dplyr::summarize(
            mean_neg = mean(neg, na.rm = TRUE),
            mean_neu = mean(neu, na.rm = TRUE),
            mean_pos = mean(pos, na.rm = TRUE),
            mean_compound = mean(compound, na.rm = TRUE),
            n = n()
        ) |>
        dplyr::arrange(
            dplyr::desc(mean_neg)
        ) |>
        # Filter out missing countries - only necessary as analysis not completed yet
        dplyr::filter(
            !is.nan(mean_neg)
        ) |>
        saveRDS(file = params$fn_sentiment_spatial_data)
}
```


```{r}
#| label: sentiment_neu_per_countries_map
#|

if (length(list.files(path = file.path("ch2_technology", "maps"), pattern = "sentiment_neu_per_countries"))< 6) {
    data <- readRDS(params$fn_sentiment_spatial_data)

    map <- data |>
        map_country_codes(
            map_type = "countries",
            values = "mean_neu",
            geodata_path = params$gdm_dir
        ) +
        ggplot2::scale_fill_gradient2(low = "#E69F00", mid = "white", high = "#56B4E9", midpoint = 0) +
        ggplot2::ggtitle("Mean neutral sentiment (0 - 1) - all countries")

    map_sel <- data |>
        dplyr::filter(n > params$min_count_sentiment_timeseries) |>
        map_country_codes(
            map_type = "countries",
            values = "mean_neu",
            geodata_path = params$gdm_dir
        ) +
        ggplot2::scale_fill_gradient2(low = "#E69F00", mid = "white", high = "#56B4E9", midpoint = 0) +
        ggplot2::ggtitle("Mean neutral sentiment (0 - 1) - more than 10 works")

    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_neu_per_countries_all.pdf"),
        width = 12,
        height = 8,
        map
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_neu_per_countries_all.svg"),
        width = 12,
        height = 8,
        map
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_neu_per_countries_all.png"),
        width = 12,
        height = 8,
        map
    )

    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_neu_per_countries_10.pdf"),
        width = 12,
        height = 8,
        map_sel
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_neu_per_countries_10.svg"),
        width = 12,
        height = 8,
        map_sel
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_neu_per_countries_10.png"),
        width = 12,
        height = 8,
        map_sel
    )

    rm(map, data)
}
```


```{r}
#| label: sentiment_pos_per_countries_map
#|

if (length(list.files(path = file.path("ch2_technology", "maps"), pattern = "sentiment_pos_per_countries"))< 6) {
    data <- readRDS(params$fn_sentiment_spatial_data)

    map <- data |>
        map_country_codes(
            map_type = "countries",
            values = "mean_pos",
            geodata_path = params$gdm_dir
        ) +
        ggplot2::scale_fill_gradient2(low = "#E69F00", mid = "white", high = "#56B4E9", midpoint = 0) +
        ggplot2::ggtitle("Mean positive sentiment (0 - 1) - all countries")

    map_sel <- data |>
        dplyr::filter(n > params$min_count_sentiment_timeseries) |>
        map_country_codes(
            map_type = "countries",
            values = "mean_pos",
            geodata_path = params$gdm_dir
        ) +
        ggplot2::scale_fill_gradient2(low = "#E69F00", mid = "white", high = "#56B4E9", midpoint = 0) +
        ggplot2::ggtitle("Mean positive sentiment (0 - 1) - more than 10 works")

    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_pos_per_countries_all.pdf"),
        width = 12,
        height = 8,
        map
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_pos_per_countries_all.svg"),
        width = 12,
        height = 8,
        map
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_pos_per_countries_all.png"),
        width = 12,
        height = 8,
        map
    )

    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_pos_per_countries_10.pdf"),
        width = 12,
        height = 8,
        map_sel
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_pos_per_countries_10.svg"),
        width = 12,
        height = 8,
        map_sel
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_pos_per_countries_10.png"),
        width = 12,
        height = 8,
        map_sel
    )

    rm(map, data)
}
```

```{r}
#| label: sentiment_neg_per_countries_map
#|
if (length(list.files(path = file.path("ch2_technology", "maps"), pattern = "sentiment_neg_per_countries"))< 6) {
    data <- readRDS(params$fn_sentiment_spatial_data)

    map <- data |>
        map_country_codes(
            map_type = "countries",
            values = "mean_neg",
            geodata_path = params$gdm_dir
        ) +
        ggplot2::scale_fill_gradient2(low = "#E69F00", mid = "white", high = "#56B4E9", midpoint = 0) +
        ggplot2::ggtitle("Mean negative sentiment (0 - 1) - all countries")

    map_sel <- data |>
        dplyr::filter(n > params$min_count_sentiment_timeseries) |>
        map_country_codes(
            map_type = "countries",
            values = "mean_pos",
            geodata_path = params$gdm_dir
        ) +
        ggplot2::scale_fill_gradient2(low = "#E69F00", mid = "white", high = "#56B4E9", midpoint = 0) +
        ggplot2::ggtitle("Mean negative sentiment (0 - 1) - more than 10 works")


    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_neg_per_countries_all.pdf"),
        width = 12,
        height = 8,
        map
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_neg_per_countries_all.svg"),
        width = 12,
        height = 8,
        map
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_neg_per_countries_all.png"),
        width = 12,
        height = 8,
        map
    )

    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_neg_per_countries_10.pdf"),
        width = 12,
        height = 8,
        map_sel
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_neg_per_countries_10.svg"),
        width = 12,
        height = 8,
        map_sel
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_neg_per_countries_10.png"),
        width = 12,
        height = 8,
        map_sel
    )

    rm(map, data)
}
```



```{r}
#| label: sentiment_compound_per_countries_map
#|

if (length(list.files(path = file.path("ch2_technology", "maps"), pattern = "sentiment_comp_per_countries"))< 6) {
    data <- readRDS(params$fn_sentiment_spatial_data)

    map <- data |>
        map_country_codes(
            map_type = "countries",
            values = "mean_compound",
            geodata_path = params$gdm_dir
        ) +
        ggplot2::scale_fill_gradient2(low = "#E69F00", mid = "white", high = "#56B4E9", midpoint = 0) +
        ggplot2::ggtitle("Mean compound sentiment (-1: negative; 1: positive) - all countries")

    map_sel <- data |>
        dplyr::filter(n > params$min_count_sentiment_timeseries) |>
        map_country_codes(
            map_type = "countries",
            values = "mean_compound",
            geodata_path = params$gdm_dir
        ) +
        ggplot2::scale_fill_gradient2(low = "#E69F00", mid = "white", high = "#56B4E9", midpoint = 0) +
        ggplot2::ggtitle("Mean compound sentiment (-1: negative; 1: positive) - more than 10 works")

    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_comp_per_countries_all.pdf"),
        width = 12,
        height = 8,
        map
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_comp_per_countries_all.svg"),
        width = 12,
        height = 8,
        map
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_comp_per_countries_all.png"),
        width = 12,
        height = 8,
        map
    )

    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_comp_per_countries_10.pdf"),
        width = 12,
        height = 8,
        map_sel
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_comp_per_countries_10.svg"),
        width = 12,
        height = 8,
        map_sel
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "maps", "sentiment_comp_per_countries_10.png"),
        width = 12,
        height = 8,
        map_sel
    )

    rm(map, data)
}
```


### Timeseries

```{r}
#| label: sentiments_over_time_data
#|

if (!file.exists(params$fn_sentiment_temporal_data)) {
    data <- readRDS(params$fn_sentiment_results) |>
        select(
            work_id = id,
            year = date,
            neg,
            neu,
            pos,
            compound
        ) |>
        dplyr::group_by(year) |>
        dplyr::summarize(
            neg = mean(neg),
            neu = mean(neu),
            pos = mean(pos),
            compound = mean(compound),
            n = n()
        ) |>
        saveRDS(file = params$fn_sentiment_temporal_data)
}
```


```{r}
#| label: sentiments_marine_over_time_data
#|

if (!file.exists(params$fn_sentiment_marine_temporal_data)) {
    data <- readRDS(params$fn_sentiment_results) |>
        select(
            work_id = id,
            year = date,
            neg,
            neu,
            pos,
            compound
        ) |>
        dplyr::filter(
            work_id %in% oa_count$vision_technology_marine_ids
        ) |>
        dplyr::group_by(year) |>
        dplyr::summarize(
            neg = mean(neg),
            neu = mean(neu),
            pos = mean(pos),
            compound = mean(compound),
            n = n()
        ) |>
        saveRDS(file = params$fn_sentiment_marine_temporal_data)
}

```

```{r}
#| label: sentiments_over_time_all_figure
#|

if (length(list.files(path = file.path("ch2_technology", "figures"), pattern = "sentiments_over_time"))< 9) {
    figure <- readRDS(params$fn_sentiment_temporal_data) |>
        dplyr::filter(
            n > params$min_count_sentiment_timeseries
        ) |>
        tidyr::pivot_longer(cols = c(neg, neu, pos, compound), names_to = "type", values_to = "value") |>
        ggplot2::ggplot() +
        ggplot2::geom_line(aes(x = year, y = value, color = type, linetype = type)) +
        ggplot2::scale_color_manual(values = c("black", "red", "blue", "green")) +
        ggplot2::labs(
            title = paste0("Sentiment Analysis Scores (n > ", params$min_count_sentiment_timeseries, ")"),
            x = "Year",
            y = "Score",
            color = "Type",
            linetype = "Type"
        ) +
        ggplot2::theme_minimal()


    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_over_time.pdf"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_over_time.svg"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_over_time.png"),
        width = 12,
        height = 6,
        figure
    )

    rm(figure)
}
```

```{r}
#| label: sentiments_over_time_table_neg_pos_figure
#|

if (length(list.files(path = file.path("ch2_technology", "figures"), pattern = "sentiments_over_time_neg_pos"))< 6) {
    figure <- readRDS(params$fn_sentiment_temporal_data) |>
        dplyr::filter(
            n > params$min_count_sentiment_timeseries
        ) |>
        tidyr::pivot_longer(cols = c(neg, pos), names_to = "type", values_to = "value") |>
        ggplot2::ggplot() +
        ggplot2::geom_line(aes(x = year, y = value, color = type, linetype = type)) +
        ggplot2::scale_color_manual(values = c("black", "red", "blue", "green")) +
        ggplot2::labs(
            title = paste0("Sentiment Analysis Scores (n > ", params$min_count_sentiment_timeseries, ")"),
            x = "Year",
            y = "Score",
            color = "Type",
            linetype = "Type"
        ) +
        ggplot2::theme_minimal()


    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_over_time_neg_pos.pdf"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_over_time_neg_pos.svg"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_over_time_neg_pos.png"),
        width = 12,
        height = 6,
        figure
    )

    rm(figure)
}
```


```{r}
#| label: sentiments_marine_over_time_table_neg_pos_figure
#|

if (length(list.files(path = file.path("ch2_technology", "figures"), pattern = "sentiments_marine_over_time_neg_pos"))< 6) {
    figure <- readRDS(params$fn_sentiment_marine_temporal_data) |>
        dplyr::filter(
            n > params$min_count_sentiment_timeseries
        ) |>
        tidyr::pivot_longer(cols = c(neg, pos), names_to = "type", values_to = "value") |>
        ggplot2::ggplot() +
        ggplot2::geom_line(aes(x = year, y = value, color = type, linetype = type)) +
        ggplot2::scale_color_manual(values = c("black", "red", "blue", "green")) +
        ggplot2::labs(
            title = paste0("Sentiment Analysis Scores (n > ", params$min_count_sentiment_timeseries, ")"),
            x = "Year",
            y = "Score",
            color = "Type",
            linetype = "Type"
        ) +
        ggplot2::theme_minimal()


    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_marine_over_time_neg_pos.pdf"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_marine_over_time_neg_pos.svg"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_marine_over_time_neg_pos.png"),
        width = 12,
        height = 6,
        figure
    )

    rm(figure)
}
```

```{r}
#| label: sentiments_neg_over_time_figure
#|

if (length(list.files(path = file.path("ch2_technology", "figures"), pattern = "sentiments_neg_over_time"))< 3) {
    figure <- readRDS(params$fn_sentiment_temporal_data) |>
        dplyr::filter(
            n > params$min_count_sentiment_timeseries
        ) |>
        ggplot2::ggplot() +
        ggplot2::geom_line(ggplot2::aes(x = year, y = neg)) +
        ggplot2::labs(
            title = paste0("Sentiment Analysis negative Score (n > ", params$min_count_sentiment_timeseries, ")"),
            x = "Year",
            y = "Negative score"
        ) +
        ggplot2::theme_minimal()

    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_neg_over_time.pdf"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_neg_over_time.svg"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_neg_over_time.png"),
        width = 12,
        height = 6,
        figure
    )

    rm(figure)
}
```

```{r}
#| label: sentiments_neu_over_time_figure
#|
if (length(list.files(path = file.path("ch2_technology", "figures"), pattern = "sentiments_neu_over_time"))< 3) {
    figure <- readRDS(params$fn_sentiment_temporal_data) |>
        dplyr::filter(
            n > params$min_count_sentiment_timeseries
        ) |>
        ggplot2::ggplot() +
        ggplot2::geom_line(ggplot2::aes(x = year, y = neu)) +
        ggplot2::labs(
            title = paste0("Sentiment Analysis neutral Score (n > ", params$min_count_sentiment_timeseries, ")"),
            x = "Year",
            y = "Neutral score"
        ) +
        ggplot2::theme_minimal()

    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_neu_over_time.pdf"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_neu_over_time.svg"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_neu_over_time.png"),
        width = 12,
        height = 6,
        figure
    )

    rm(figure)
}
```


```{r}
#| label: sentiments_pos_over_time_figure
#|

if (length(list.files(path = file.path("ch2_technology", "figures"), pattern = "sentiments_pos_over_time"))< 3) {
    figure <- readRDS(params$fn_sentiment_temporal_data) |>
        dplyr::filter(
            n > params$min_count_sentiment_timeseries
        ) |>
        ggplot2::ggplot() +
        ggplot2::geom_line(ggplot2::aes(x = year, y = pos)) +
        ggplot2::labs(
            title = paste0("Sentiment Analysis positive Score (n > ", params$min_count_sentiment_timeseries, ")"),
            x = "Year",
            y = "Positive score"
        ) +
        ggplot2::theme_minimal()

    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_pos_over_time.pdf"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_pos_over_time.svg"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_pos_over_time.png"),
        width = 12,
        height = 6,
        figure
    )

    rm(figure)
}
```


```{r}
#| label: sentiments_comp_over_time_figure
#|
if (length(list.files(path = file.path("ch2_technology", "figures"), pattern = "sentiments_comp_over_time"))< 3) {
    figure <- readRDS(params$fn_sentiment_temporal_data) |>
        dplyr::filter(
            n > params$min_count_sentiment_timeseries
        ) |>
        ggplot2::ggplot() +
        ggplot2::geom_line(ggplot2::aes(x = year, y = compound)) +
        ggplot2::labs(
            title = paste0("Sentiment Analysis Compound Score (n > ", params$min_count_sentiment_timeseries, ")"),
            x = "Year",
            y = "Compound score"
        ) +
        ggplot2::theme_minimal()

    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_comp_over_time.pdf"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_comp_over_time.svg"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_comp_over_time.png"),
        width = 12,
        height = 6,
        figure
    )

    rm(figure)
}
```

```{r}
#| label: sentiments_marine_over_time_neg_pos_ridgeplot_figure
#|

if (length(list.files(path = file.path("ch2_technology", "figures"), pattern = "sentiments_marine_over_time_neg_pos_ridge"))< 3) {
    figure <- readRDS(params$fn_sentiment_results) |>
        select(
            work_id = id,
            year = date,
            neg,
            neu,
            pos,
            compound
        ) |>
        dplyr::ungroup() |>
        dplyr::filter(
            work_id %in% oa_count$vision_technology_marine_ids
        ) |>
        dplyr::filter(
            year >= 1980,
            year < 2025
        ) |>
        mutate(
            year = as.factor(year)
        ) |>
        ggplot() +
        geom_density_ridges(aes(x = pos, y = year, fill = "positive"), rel_min_height = 0.005) +
        geom_density_ridges(aes(x = -neg, y = year, fill = "negative"), rel_min_height = 0.005)

    # Calculate the number of points per year
    counts <- figure$data %>%
        group_by(year) %>%
        summarise(
            n = n()
        ) |>
        mutate(
            n = paste0("N = ", n)
        )


    # Add the number of points per year as text
    figure <- figure +
        geom_label(
            data = counts,
            aes(
                x = -Inf,
                y = year,
                label = sprintf("%-10s", n)
            ),
            size = rel(3.3),
            hjust = "inward",
            vjust = 0
        )



    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_marine_over_time_neg_pos_ridge.pdf"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_marine_over_time_neg_pos_ridge.svg"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_marine_over_time_neg_pos_ridge.png"),
        width = 12,
        height = 6,
        figure
    )

    rm(figure)
}
```

```{r}
#| label: sentiments_over_time_neg_pos_ridgeplot_figure
#|

if (length(list.files(path = file.path("ch2_technology", "figures"), pattern = "sentiments_over_time_neg_pos_ridge"))< 3) {
    figure <- readRDS(params$fn_sentiment_results) |>
        select(
            work_id = id,
            year = date,
            neg,
            neu,
            pos,
            compound
        ) |>
        dplyr::ungroup() |>
        dplyr::filter(
            year >= 1980,
            year < 2025
        ) |>
        mutate(
            year = as.factor(year)
        ) |>
        ggplot() +
        geom_density_ridges(aes(x = pos, y = year, fill = "positive"), rel_min_height = 0.005) +
        geom_density_ridges(aes(x = -neg, y = year, fill = "negative"), rel_min_height = 0.005)

    # Calculate the number of points per year
    counts <- figure$data %>%
        group_by(year) %>%
        summarise(
            n = n()
        ) |>
        mutate(
            n = paste0("N = ", n)
        )

    # Add the number of points per year as text
    figure <- figure +
        geom_text(
            data = counts, 
            aes(
                x = -Inf,
                y = year, 
                label = sprintf("%-10s", n)
            ), 
            size = rel(3.3),
            hjust = "inward", 
            vjust = 0
    )



    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_over_time_neg_pos_ridge.pdf"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_over_time_neg_pos_ridge.svg"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_over_time_neg_pos_ridge.png"),
        width = 12,
        height = 6,
        figure
    )

    rm(figure)
}

```{r}
#| label: sentiments_marine_over_time_neg_pos_figure
#|

if (length(list.files(path = file.path("ch2_technology", "figures"), pattern = "sentiments_over_time_neg_pos")) < 3) {
    figure <- readRDS(params$fn_sentiment_temporal_data) |>
        dplyr::filter(
            n > params$min_count_sentiment_timeseries
        ) |>
        tidyr::pivot_longer(cols = c(neg, pos), names_to = "type", values_to = "value") |>
        ggplot2::ggplot() +
        ggplot2::geom_line(aes(x = year, y = value, color = type, linetype = type)) +
        ggplot2::scale_color_manual(values = c("black", "red", "blue", "green")) +
        ggplot2::labs(
            title = paste0("Sentiment Analysis Scores (n > ", params$min_count_sentiment_timeseries, ")"),
            x = "Year",
            y = "Score",
            color = "Type",
            linetype = "Type"
        ) +
        ggplot2::theme_minimal()


    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_over_time_neg_pos.pdf"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_over_time_neg_pos.svg"),
        width = 12,
        height = 6,
        figure
    )
    ggplot2::ggsave(
        file.path("ch2_technology", "figures", "sentiments_over_time_neg_pos.png"),
        width = 12,
        height = 6,
        figure
    )

    rm(figure)
}
```


## Extract `marine papers with sentiment scores

```{r}
#| label: extract_marine_papers
#|

if (!file.exists(params$fn_marine_sentiment_all)) {
    corpus_read(params$corpus_dir) |>
        dplyr::filter(
            id %in% oa_count$vision_technology_marine_ids
        ) |>
        dplyr::select(id, doi, author_abbr, display_name, ab) |>
        dplyr::rename(abstract = ab, title = display_name) |>
        dplyr::collect() |>
        dplyr::mutate(
            abstract = substr(abstract, start = 1, stop = 5000)
        ) |>
        dplyr::left_join(
            y = readRDS(params$fn_sentiment_results) |>
                dplyr::select(
                    id,
                    neg,
                    neu,
                    pos,
                    compound
                ),
            by = "id"
        ) |>
        writexl::write_xlsx(path = params$fn_marine_sentiment_all)
}

if (!file.exists(params$fn_marine_sentiment)) {
    corpus_read(params$corpus_dir) |>
        dplyr::filter(
            id %in% oa_count$vision_technology_marine_ids
        ) |>
        dplyr::select(id, doi, author_abbr, display_name, ab) |>
        dplyr::rename(abstract = ab, title = display_name) |>
        dplyr::collect() |>
        dplyr::slice_sample(n = 250) |>
        dplyr::mutate(
            abstract = substr(abstract, start = 1, stop = 5000)
        ) |>
        dplyr::left_join(
            y = readRDS(params$fn_sentiment_results) |>
                dplyr::select(
                    id,
                    neg,
                    neu,
                    pos,
                    compound
                ),
            by = "id"
        ) |>
        writexl::write_xlsx(path = params$fn_marine_sentiment)
}

```


# Results

The results are based on data downloaded or accessed at:

```{r}
#| label: corpus_time
#| 

c_time <- list.files(
    path = params$pages_dir,
    recursive = TRUE, 
    pattern = ".rds$", 
    full.names = TRUE
) |>
    file.mtime() |>
    as.Date() |>
    unique()
```

- **Technology Corpus**:  located at ``r params$corpus_dir`` downloaded at **`r c_time`** from [OpenAlex](https://openalex.org)
- **Counts**: obtained from at **`r oa_count$timestamp |> as.Date()`** from [OpenAlex](https://openalex.org)


## `vision AND technology` in TCA Corpus

For the TCA Corpus, we do have 
**`r read_corpus(params$corpus_dir) |> nrow() |> formatC(format = "f", big.mark = ",", digits = 0)`** number of works.


## Random Sample of 250 works
An Excel file conataining a random sample of 250 works from the Technology Corpus (`technology` AND `vision` AND `nature` AND `transformativechange`) with the fields `id`, `doi`, `author_abbr` and `abstract` of the papers. The Excel file can be downloaded from  [here](params$fn_random_sample_250){target=_blank}. 

## Subfields

The subfields are based on the main topic assigned to each work. There are other topics also assigned, but this one has been identified as the main topic by an algorythm. `count` is the number of works in the `vision AND technology` corpus which have been assigned to the subfield.

**Please take a look at these subfields of the topics to identify the ones to be filtered out.**

The easies would be to download the Excel file through the button and to mark the subfields to be filtered out.
```{r}
IPBES.R::table_dt((oa_count$vision_technology_subfields |> dplyr::arrange(desc(count))), fixedColumns = NULL, fn = "Vision Technology Subfields")
```

## Works in Technology Corpus also in Cases Corpus

```{r}
#| label: technology_in_cases
#|

cases_id <- corpus_read(params$corpus_cases_dir) |>
    dplyr::select(id) |>
    dplyr::distinct(id) |>
    collect() |>
    unlist() |>
    as.vector()

tech_id <- corpus_read(params$corpus_dir) |>
    dplyr::select(id) |>
    dplyr::distinct(id) |>
    collect() |>
    unlist() |>
    as.vector()

count_cases <- sum(tech_id %in% cases_id)
count <- length(tech_id)
p_cases <- count_cases / count

count_marine <- length(oa_count$vision_technology_marine_ids)
count_marine_cases <- sum(oa_count$vision_technology_marine_ids %in% cases_id)
p_marine_cases <- count_marine_cases / count_marine

```

Out of the **`r count`** works in the Technology Corpus, **`r count_cases`** are also in the Cases Corpus. This is **`r round(p_cases * 100, 2)`%** of the Technology Corpus.

Out of the **`r count_marine`** marine works in the Technology Corpus, **`r count_marine_cases`** are also in the Cases Corpus. This is **`r round(p_marine_cases * 100, 2)`%** of the Technology Corpus.

## Publications over time

![Publications over time of different corpora](ch2_technology/figures/publications_over_time.png)

A pdf of the graph can be downloaded [here](ch2_technology/figures/publications_over_time.pdf){target=_blank}.

![Publications over time of different corpora](ch2_technology/figures/publications_over_time_yearly.png)

A pdf of the graph can be downloaded [here](ch2_technology/figures/publications_over_time_yearly.pdf){target=_blank}.


## Sentiment Analysis

Two `.parquet` files containing the `id`, `publication_year` and `ab` (abstract) were extracted and are available upon request due to their size.

For analyzing the sentiments of the provided abstracts, we have used the Python NLTK package, and VADER (Valence Aware Dictionary for Sentiment Reasoning) which is an NLTK module that provides sentiment scores based on the words used. VADER is a pre-trained, rule-based sentiment analysis model in which the terms are generally labeled as per their semantic orientation as either positive or negative.

The main advantage/reason for using this model was that it doesn't require a labbed training dataset.
The output of the model is 4 statistical scores: 

- **compound**: composite score that summarizes the overall sentiment of the text, where scores close to 1 indicate a positive sentiment, scores close to -1 indicate a negative sentiment, and scores close to 0 indicate a neutral sentiment
- **negative**: percentage of negative sentiments in the text 
- **neutral**: percentage of neutral sentiments in the text
- **positive**: percentage of positive sentiments in the text

```{r}
#| label: sentiment_analysis_table
#|

readRDS(params$fn_sentiment_results) |>
    IPBES.R::table_dt(fn = "sentiment_scores", fixedColumns = list(leftColumns = 2))
```

Here is the per country table

```{r}
#| label: sentiments_countries_table
#|

readRDS(params$fn_sentiment_spatial_data) |>
    IPBES.R::table_dt(fn = "sentiments_comp_over_time")
rm(data)
```

### Sentiments Over Time

This graphs shows the sentiment scores of the sentiment analysis over time.


![](`r file.path("ch2_technology", "figures", "sentiments_over_time.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "figures", "sentiments_over_time.pdf")`){target="_blank"}


For clarity, here only the positive and egative sentiments.

![](`r file.path("ch2_technology", "figures", "sentiments_over_time_neg_pos.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "figures", "sentiments_over_time_neg_pos.pdf")`){target="_blank"}


### Negative Sentiment

#### Over Time

This graphs shows the **negative score** of the sentiment analysis over time. It only 


![](`r file.path("ch2_technology", "figures", "sentiments_neg_over_time.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "figures", "sentiments_neg_over_time.pdf")`){target="_blank"}


#### Per country

![](`r file.path("ch2_technology", "maps", "sentiment_neg_per_countries_all.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "maps", "sentiment_neg_per_countries_all.pdf")`){target="_blank"}

![](`r file.path("ch2_technology", "maps", "sentiment_neg_per_countries_10.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "maps", "sentiment_neg_per_countries_10.pdf")`){target="_blank"}


### Neutral Sentiment

#### Over Time

This graphs shows the **compound score** of the sentiment analysis over time. It only 



![](`r file.path("ch2_technology", "figures", "sentiments_neu_over_time.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "figures", "sentiments_neu_over_time.pdf")`){target="_blank"}


#### Per country


![](`r file.path("ch2_technology", "maps", "sentiment_neu_per_countries_all.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "maps", "sentiment_neu_per_countries_all.pdf")`){target="_blank"}

![](`r file.path("ch2_technology", "maps", "sentiment_neu_per_countries_10.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "maps", "sentiment_neu_per_countries_10.pdf")`){target="_blank"}




### Positive Sentiment

#### Over Time

This graphs shows the **compound score** of the sentiment analysis over time. It only 

![](`r file.path("ch2_technology", "figures", "sentiments_pos_over_time.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "figures", "sentiments_pos_over_time.pdf")`){target="_blank"}

#### Per country

![](`r file.path("ch2_technology", "maps", "sentiment_pos_per_countries_all.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "maps", "sentiment_pos_per_countries_all.pdf")`){target="_blank"}

![](`r file.path("ch2_technology", "maps", "sentiment_pos_per_countries_10.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "maps", "sentiment_neu_per_countries_10.pdf")`){target="_blank"}




### Compound Sentiment

#### Over Time

This graphs shows the **compound score** of the sentiment analysis over time. It only 

![](`r file.path("ch2_technology", "figures", "sentiments_comp_over_time.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "figures", "sentiments_comp_over_time.pdf")`){target="_blank"}

#### Per country


![](`r file.path("ch2_technology", "maps", "sentiment_comp_per_countries_all.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "maps", "sentiment_comp_per_countries_all.pdf")`){target="_blank"}

![](`r file.path("ch2_technology", "maps", "sentiment_comp_per_countries_10.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "maps", "sentiment_comp_per_countries_10.pdf")`){target="_blank"}

### Marine Vision and Technology Sentiments

#### Figures

This graphs shows the **positive and negative score** of the sentiment analysis over time.

![](`r file.path("ch2_technology", "figures", "sentiments_marine_over_time_neg_pos.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "figures", "sentiments_marine_over_time_neg_pos.pdf")`){target="_blank"}

![](`r file.path("ch2_technology", "figures", "sentiments_marine_over_time_neg_pos_ridge.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "figures", "sentiments_marine_over_time_neg_pos_ridge.pdf")`){target="_blank"}

#### Download of Corpus
An Excel file conataining the works from the Narine subset of the Technology Corpus (`technology` AND `vision` AND `nature` AND `transformativechange` AND `marine`) with the fields `id`, `doi`, `author_abbr` and `abstract` as well as the sentiment scores of the papers. 

There are technical issues with this at the moment.


The Excel file with a random subset of 250 papers can be downloaded from  [here](`r params$fn_marine_sentiment`){target=_blank}. 

The Excel file with all papers can be downloaded from  [here](`r params$fn_marine_sentiment_all`){target=_blank}.


### Additional Ridge plot

![](`r file.path("ch2_technology", "figures", "sentiments_over_time_neg_pos_ridge.png")`)

To download high resolution, [click here](`r file.path("ch2_technology", "figures", "sentiments_over_time_neg_pos_ridge.pdf")`){target="_blank"}
